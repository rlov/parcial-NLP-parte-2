{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RsctffnJGf6"
      },
      "source": [
        "# OLIVARES VENTURA RICARDO LEONARDO\n",
        "# CÓDIGO: 20192002A\n",
        "# EXAMEN PARCIAL DE PROCESAMIENTO DEL LENGUAJE NATURAL - VERSIÓN 2 - FINAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZjIPIiEKvop"
      },
      "source": [
        "# 0.1.1 Ejercicio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGmpf6fuLAaz"
      },
      "source": [
        "## Parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt5Zla5bKz5z"
      },
      "source": [
        "Dadas tres oraciones \"all models are wrong\", a model is wrong y some models are useful, y\n",
        "un vocabulario {< s >, < /s >, a, all, are, model, models, some, useful, wrong}. En codigo responde las siguientes preguntas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QauYHUpBJALt",
        "outputId": "3f4f854f-b8e3-40dd-8d0f-381f9573a0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a) Probabilidad de todos los bigramas sin suavizado:  {('all', 'models'): 1.0, ('models', 'are'): 1.0, ('are', 'wrong'): 0.5, ('a', 'model'): 1.0, ('model', 'is'): 1.0, ('is', 'wrong'): 1.0, ('some', 'models'): 1.0, ('are', 'useful'): 0.5}\n",
            "b.1) Probabilidad de todos los bigramas son suavizado de laplace o add-one:  {('all', 'models'): 0.2, ('models', 'are'): 0.2727272727272727, ('are', 'wrong'): 0.18181818181818182, ('a', 'model'): 0.2, ('model', 'is'): 0.2, ('is', 'wrong'): 0.2, ('some', 'models'): 0.2, ('are', 'useful'): 0.18181818181818182}\n",
            "b.2) Probabilidad del bigrama no visto 'a models' es:  0.1\n",
            "c.1) Probabilidad de todos los bigramas son suavizado add-k = 0.05 son:  {('all', 'models'): 0.7241379310344828, ('models', 'are'): 0.8367346938775508, ('are', 'wrong'): 0.42857142857142855, ('a', 'model'): 0.7241379310344828, ('model', 'is'): 0.7241379310344828, ('is', 'wrong'): 0.7241379310344828, ('some', 'models'): 0.7241379310344828, ('are', 'useful'): 0.42857142857142855}\n",
            "c.1) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.05 es:  0.034482758620689655\n",
            "c.2) Probabilidad de todos los bigramas son suavizado add-k = 0.15 son:  {('all', 'models'): 0.4893617021276596, ('models', 'are'): 0.6417910447761195, ('are', 'wrong'): 0.34328358208955223, ('a', 'model'): 0.4893617021276596, ('model', 'is'): 0.4893617021276596, ('is', 'wrong'): 0.4893617021276596, ('some', 'models'): 0.4893617021276596, ('are', 'useful'): 0.34328358208955223}\n",
            "c.2) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.15 es:  0.06382978723404256\n",
            "d.1) Probabilidad de todos los bigramas backoff:  {('all', 'models'): 1.0, ('models', 'are'): 1.0, ('are', 'wrong'): 0.5, ('a', 'model'): 1.0, ('model', 'is'): 1.0, ('is', 'wrong'): 1.0, ('some', 'models'): 1.0, ('are', 'useful'): 0.5}\n",
            "d.1) Probabilidad del bigrama no visto 'a models' es:  0.2222222222222222\n"
          ]
        }
      ],
      "source": [
        "corpus = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "vocabulary = {\n",
        "    \"<s/>\",\n",
        "    \"a\",\n",
        "    \"all\",\n",
        "    \"are\",\n",
        "    \"model\",\n",
        "    \"models\",\n",
        "    \"some\",\n",
        "    \"useful\",\n",
        "    \"wrong\"\n",
        "}\n",
        "\n",
        "# a) Calcula las probabilidades de todos los bigramas sin suavizado\n",
        "# Preprocesamos el texto y convertimos todo a minúscula\n",
        "# Para poder estandarizar y que no detecte como dos palabras\n",
        "# distintas por ser mayúscula y munúscula\n",
        "def preprocess_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def calculate_bigram_probabilities(corpus):\n",
        "    bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "    # que aparezca en el corpus\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "            unigram = tokens[i]\n",
        "            total_unigrams += 1\n",
        "            unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram = (unigram, tokens[i + 1])\n",
        "                bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        unigram = bigram[0]\n",
        "        # Aquí calculamos la probabilidad del bigrama, la cual es\n",
        "        # la cantidad de veces que aparece un bigrama entre el unigrama que precede al último elemento\n",
        "        # del bigrama\n",
        "        bigram_probabilities[bigram] = count / unigram_counts[unigram]\n",
        "\n",
        "    return bigram_probabilities\n",
        "\n",
        "bigram_probabilities = calculate_bigram_probabilities(corpus)\n",
        "\n",
        "print(\"a) Probabilidad de todos los bigramas sin suavizado: \", bigram_probabilities)\n",
        "\n",
        "# b) Calcule las probabilidades de todos los bigramas y el bigrama no visto a models con suavizado add-one.\n",
        "bigrama_doesnt_exist = (\"a\", \"models\")\n",
        "\n",
        "# Aquí aplicaremos el suavizado add-one o también llamado suavizado de Laplace,\n",
        "# el cual es útil cuando hay bigramas que no existen en el corpus de entrenamiento\n",
        "# y por ende se le asigna una probabilidad de cero, pero justamente este suavizado\n",
        "# lo que hace es que suma 1 al numerador y |v| (tamaño del vocabulario) al denominador\n",
        "# haciendo que los bigramas que no existan en el corpus de entrenamiento tengan una probabilidad\n",
        "# distinta de cero\n",
        "\n",
        "def generate_n_grams(corpus, n):\n",
        "    n_grams = []\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            n_gram = tuple(tokens[i:i+n])\n",
        "            n_grams.append(n_gram)\n",
        "    return n_grams\n",
        "\n",
        "def calculate_bigram_probabilities_with_add_one(corpus, vocabulary, word1, word2):\n",
        "    bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "    # que aparezca en el corpus\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "            unigram = tokens[i]\n",
        "            total_unigrams += 1\n",
        "            unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram = (unigram, tokens[i + 1])\n",
        "                bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        unigram = bigram[0]\n",
        "        # Aquí calculamos la probabilidad del bigrama aplicando el\n",
        "        # suavizado de Laplace\n",
        "        bigram_probabilities[bigram] = (count + 1) / (unigram_counts[unigram] + len(vocabulary))\n",
        "\n",
        "        if word1 != \"\" and word2 != \"\":\n",
        "          unigram_count = unigram_counts[(word1)]\n",
        "          return (0 + 1) / (unigram_count + len(vocabulary))\n",
        "    return bigram_probabilities\n",
        "\n",
        "print(\"b.1) Probabilidad de todos los bigramas son suavizado de laplace o add-one: \",calculate_bigram_probabilities_with_add_one(corpus, vocabulary, \"\", \"\"))\n",
        "print(\"b.2) Probabilidad del bigrama no visto 'a models' es: \",calculate_bigram_probabilities_with_add_one(corpus, vocabulary, \"a\", \"models\"))\n",
        "\n",
        "# c) Calcule las probabilidades de todos los bigramas y el bigrama no visto a models con suavizado add-k. Pruebe k = 0.05 y k =0.15.\n",
        "\n",
        "def calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, k, word1, word2):\n",
        "    bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "    # que aparezca en el corpus\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "    # k = 0.15\n",
        "\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "            unigram = tokens[i]\n",
        "            total_unigrams += 1\n",
        "            unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram = (unigram, tokens[i + 1])\n",
        "                bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        unigram = bigram[0]\n",
        "        # Aquí calculamos la probabilidad del bigrama aplicando el\n",
        "        # suavizado add-k\n",
        "        bigram_probabilities[bigram] = (count + k) / (unigram_counts[unigram] + k * len(vocabulary))\n",
        "        if word1 != \"\" and word2 != \"\":\n",
        "          unigram_count = unigram_counts[(word1)]\n",
        "          return (0 + k) / (unigram_count + k * len(vocabulary))\n",
        "    return bigram_probabilities\n",
        "\n",
        "print(\"c.1) Probabilidad de todos los bigramas son suavizado add-k = 0.05 son: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.05, \"\", \"\"))\n",
        "print(\"c.1) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.05 es: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.05, \"a\", \"models\"))\n",
        "print(\"c.2) Probabilidad de todos los bigramas son suavizado add-k = 0.15 son: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.15, \"\", \"\"))\n",
        "print(\"c.2) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.15 es: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.15, \"a\", \"models\"))\n",
        "\n",
        "# d) Calcula las probabilidades de todo los bigramas y el bigrama\n",
        "# no visto a models con back-off y stupid-backoff\n",
        "\n",
        "def calculate_bigram_probabilities_with_backoff(corpus, vocabulary, word1, word2):\n",
        "  bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "  # que aparezca en el corpus\n",
        "  unigram_counts = {}\n",
        "  total_unigrams = 0\n",
        "  # k = 0.15\n",
        "\n",
        "  for sentence in corpus:\n",
        "      tokens = preprocess_text(sentence)\n",
        "      for i in range(len(tokens)):\n",
        "          unigram = tokens[i]\n",
        "          total_unigrams += 1\n",
        "          unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "          if i < len(tokens) - 1:\n",
        "              bigram = (unigram, tokens[i + 1])\n",
        "              bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "  bigram_probabilities = {}\n",
        "  for bigram, count in bigram_counts.items():\n",
        "      unigram = bigram[0]\n",
        "      # Aquí calculamos la probabilidad del bigrama aplicando el\n",
        "      # suavizado add-k\n",
        "      if count > 0:\n",
        "        bigram_probabilities[bigram] = count/unigram_counts[unigram]\n",
        "      else:\n",
        "        bigram_probabilities[bigram] = unigram_counts[bigram[1]] / len(vocabulary)\n",
        "\n",
        "      if word1 != \"\" and word2 != \"\":\n",
        "        return unigram_counts[bigram[1]] / len(vocabulary)\n",
        "  return bigram_probabilities\n",
        "\n",
        "print(\"d.1) Probabilidad de todos los bigramas backoff: \",calculate_bigram_probabilities_with_backoff(corpus, vocabulary, \"\", \"\"))\n",
        "print(\"d.1) Probabilidad con back-off del bigrama no visto 'a models' es: \",calculate_bigram_probabilities_with_backoff(corpus, vocabulary, \"a\", \"models\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdZLmmaLFuM"
      },
      "source": [
        "## Parte 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY4S7qiaX6zw",
        "outputId": "b1461872-0447-4f17-cff6-7cfc02434980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigramas N_r y r_star: \n",
            "r: 1, N_r: 6, r_star: 1.0\n",
            "r: 2, N_r: 3, r_star: N/A\n"
          ]
        }
      ],
      "source": [
        "# e) El suavizado de Good-Turing reasigna la masa de\n",
        "# probabilidad de los n-gramas ricos a los n-gramas pobres.\n",
        "# Dado un corpus D, supongamos que tratamos todas las unigrama\n",
        "# desconocidos como (UNK), por lo tanto, el vocabulario es\n",
        "# {w: w e D} U {<UNK>} y No = 1. Calcula r, Nr para todas las unigramas de la parte 1\n",
        "\n",
        "corpus = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "vocabulary = {\n",
        "    \"<s/>\",\n",
        "    \"a\",\n",
        "    \"all\",\n",
        "    \"are\",\n",
        "    \"model\",\n",
        "    \"models\",\n",
        "    \"some\",\n",
        "    \"useful\",\n",
        "    \"wrong\"\n",
        "}\n",
        "\n",
        "def n_grams_with_good_turing():\n",
        "  unigram_counts = {}\n",
        "  total_unigrams = 0\n",
        "  for sentence in corpus:\n",
        "      tokens = preprocess_text(sentence)\n",
        "      for i in range(len(tokens)):\n",
        "          unigram = tokens[i]\n",
        "          total_unigrams += 1\n",
        "          unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "\n",
        "  counts_frequency = {}\n",
        "  for count in unigram_counts.values():\n",
        "    counts_frequency[count] = counts_frequency.get(count, 0) + 1\n",
        "\n",
        "  # Calculamos el N_r y r\n",
        "  N_r = {}\n",
        "  for r in counts_frequency:\n",
        "    N_r[r] = counts_frequency[r]\n",
        "\n",
        "  # Y calculamos r_star y N_r_star\n",
        "  r_star = {}\n",
        "  N_r_star = {}\n",
        "  for r in sorted(N_r.keys()):\n",
        "    if r + 1 in N_r:\n",
        "      r_star[r] = (r + 1) * N_r.get(r + 1, 0) / N_r.get(r, 0)\n",
        "      N_r_star[r] = N_r.get(r, 0)\n",
        "\n",
        "  print(\"Unigramas N_r y r_star: \")\n",
        "  for r, N_r_value in N_r.items():\n",
        "    print(f\"r: {r}, N_r: {N_r_value}, r_star: {r_star.get(r, 'N/A')}\")\n",
        "\n",
        "n_grams_with_good_turing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwPz78R7X1tM"
      },
      "source": [
        "# 0.2 Brown clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "iuSbQrBOYnSl",
        "outputId": "67caf7ce-391f-4fa4-a2f1-bfa756de8b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Package europarl_raw is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Reanudación', 'del', 'período', 'de', 'sesiones', 'Declaro', 'reanudado', 'el', 'período', 'de', 'sesiones', 'del', 'Parlamento', 'Europeo', ',', 'interrumpido', 'el', 'viernes', '17', 'de', 'diciembre', 'pasado', ',', 'y', 'reitero', 'a', 'Sus', 'Señorías', 'mi', 'deseo', 'de', 'que', 'hayan', 'tenido', 'unas', 'buenas', 'vacaciones', '.'], ['Como', 'todos', 'han', 'podido', 'comprobar', ',', 'el', 'gran', '\"', 'efecto', 'del', 'año', '2000', '\"', 'no', 'se', 'ha', 'producido', '.'], ['En', 'cambio', ',', 'los', 'ciudadanos', 'de', 'varios', 'de', 'nuestros', 'países', 'han', 'sido', 'víctimas', 'de', 'catástrofes', 'naturales', 'verdaderamente', 'terribles', '.'], ['Sus', 'Señorías', 'han', 'solicitado', 'un', 'debate', 'sobre', 'el', 'tema', 'para', 'los', 'próximos', 'días', ',', 'en', 'el', 'curso', 'de', 'este', 'período', 'de', 'sesiones', '.'], ['A', 'la', 'espera', 'de', 'que', 'se', 'produzca', ',', 'de', 'acuerdo', 'con', 'muchos', 'colegas', 'que', 'me', 'lo', 'han', 'pedido', ',', 'pido', 'que', 'hagamos', 'un', 'minuto', 'de', 'silencio', 'en', 'memoria', 'de', 'todas', 'las', 'víctimas', 'de', 'las', 'tormentas', ',', 'en', 'los', 'distintos', 'países', 'de', 'la', 'Unión', 'Europea', 'afectados', '.'], ['Invito', 'a', 'todos', 'a', 'que', 'nos', 'pongamos', 'de', 'pie', 'para', 'guardar', 'un', 'minuto', 'de', 'silencio', '.'], ['(', 'El', 'Parlamento', ',', 'de', 'pie', ',', 'guarda', 'un', 'minuto', 'de', 'silencio', ')', 'Señora', 'Presidenta', ',', 'una', 'cuestión', 'de', 'procedimiento', '.'], ['Sabrá', 'usted', 'por', 'la', 'prensa', 'y', 'la', 'televisión', 'que', 'se', 'han', 'producido', 'una', 'serie', 'de', 'explosiones', 'y', 'asesinatos', 'en', 'Sri', 'Lanka', '.'], ['Una', 'de', 'las', 'personas', 'que', 'recientemente', 'han', 'asesinado', 'en', 'Sri', 'Lanka', 'ha', 'sido', 'al', 'Sr.', 'Kumar', 'Ponnambalam', ',', 'quien', 'hace', 'pocos', 'meses', 'visitó', 'el', 'Parlamento', 'Europeo', '.'], ['¿', 'Sería', 'apropiado', 'que', 'usted', ',', 'Señora', 'Presidenta', ',', 'escribiese', 'una', 'carta', 'al', 'Presidente', 'de', 'Sri', 'Lanka', 'expresando', 'las', 'condolencias', 'del', 'Parlamento', 'por', 'esa', 'y', 'otras', 'muertes', 'violentas', ',', 'pidiéndole', 'que', 'haga', 'todo', 'lo', 'posible', 'para', 'encontrar', 'una', 'reconciliación', 'pacífica', 'ante', 'la', 'extremadamente', 'difícil', 'situación', 'que', 'está', 'viviendo', 'su', 'país', '?'], ['Sí', ',', 'señor', 'Evans', ',', 'pienso', 'que', 'una', 'iniciativa', 'como', 'la', 'que', 'usted', 'acaba', 'de', 'sugerir', 'sería', 'muy', 'adecuada', '.'], ['Si', 'la', 'Asamblea', 'está', 'de', 'acuerdo', ',', 'haré', 'lo', 'que', 'el', 'señor', 'Evans', 'acaba', 'de', 'sugerir', '.', 'Señora', 'Presidenta', ',', 'una', 'cuestión', 'de', 'procedimiento', '.'], ['Me', 'gustaría', 'que', 'me', 'asesorara', 'sobre', 'el', 'Artículo', '143', 'concerniente', 'a', 'la', 'inadmisibilidad', '.'], ['Mi', 'pregunta', 'se', 'refiere', 'a', 'un', 'asunto', 'del', 'que', 'se', 'hablará', 'el', 'jueves', ',', 'día', 'que', 'en', 'volveré', 'a', 'plantearla', '.'], ['El', 'informe', 'Cunha', 'sobre', 'los', 'programas', 'de', 'dirección', 'plurianual', 'se', 'presenta', 'al', 'Parlamento', 'el', 'jueves', 'y', 'contiene', 'una', 'propuesta', 'en', 'el', 'apartado', '6', 'en', 'torno', 'a', 'una', 'forma', 'de', 'penalizaciones', 'basada', 'en', 'cuotas', 'que', 'debe', 'aplicarse', 'a', 'los', 'países', 'que', 'no', 'cumplan', 'anualmente', 'sus', 'objetivos', 'de', 'reducción', 'de', 'flota', '.'], ['El', 'informe', 'estipula', 'que', 'se', 'debe', 'aplicarse', 'a', 'pesar', 'del', 'principio', 'de', 'estabilidad', 'relativa', '.'], ['Creo', 'que', 'el', 'principio', 'de', 'estabilidad', 'relativa', 'es', 'un', 'principio', 'legal', 'fundamental', 'de', 'las', 'políticas', 'pesqueras', 'comunitarias', ',', 'por', 'lo', 'que', 'una', 'propuesta', 'que', 'lo', 'subvierta', 'es', 'legalmente', 'inadmisible', '.'], ['Quiero', 'saber', 'si', 'se', 'puede', 'hacer', 'este', 'tipo', 'de', 'objeción', 'a', 'lo', 'que', 'sólo', 'es', 'un', 'informe', ',', 'no', 'una', 'propuesta', 'legislativa', ',', 'y', 'si', 'es', 'algo', 'que', 'puedo', 'plantear', 'el', 'jueves', '.'], ['Su', 'Señoría', ',', 'si', 'así', 'lo', 'desea', ',', 'podrá', 'plantear', 'esta', 'cuestión', 'en', 'ese', 'momento', ',', 'es', 'decir', ',', 'el', 'jueves', 'antes', 'de', 'que', 'se', 'presente', 'el', 'informe', '.'], ['Señora', 'Presidenta', ',', 'coincidiendo', 'con', 'el', 'primer', 'período', 'parcial', 'de', 'sesiones', 'de', 'este', 'año', 'del', 'Parlamento', 'Europeo', ',', 'lamentablemente', ',', 'en', 'los', 'Estados', 'Unidos', ',', 'en', 'Texas', ',', 'se', 'ha', 'fijado', 'para', 'el', 'próximo', 'jueves', 'la', 'ejecución', 'de', 'un', 'condenado', 'a', 'la', 'pena', 'capital', ',', 'un', 'joven', 'de', '34', 'años', 'que', 'llamaremos', 'con', 'el', 'nombre', 'de', 'Hicks', '.']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-d7dc5f75b9ee>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-d7dc5f75b9ee>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-d7dc5f75b9ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "# El brown clusteing es un método de agrupamiento jerárquico\n",
        "# eque agrupo palabras basándose en eln contexto en el que\n",
        "# aparecen. Las palabras que aparecen en contextos similares tienden a tener significado similares\n",
        "# El objetivo es maximizar la probabilidada del corpus bajo un modelo\n",
        "# de lenguajge bigrama\n",
        "\n",
        "# El objetivo es encontar la asignación de clases que maximice\n",
        "# la verosimiiltud del corpus\n",
        "\n",
        "'''0.6.5. Ejercicio:'''\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "# Descargarmos algunos corpus\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('europarl_raw')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# nltk.download('cess_esp') # Este corpus está en español\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import cess_esp\n",
        "from nltk.corpus import europarl_raw\n",
        "from nltk.cluster import KMeansClusterer\n",
        "from nltk.cluster import euclidean_distance\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "# En este caso utilizaremos el corpus europarl en español\n",
        "europarl_sentences = europarl_raw.spanish.sents()\n",
        "\n",
        "print(europarl_sentences[:20])\n",
        "\n",
        "def preprocess_sentences(sentences):\n",
        "    preprocessed = []\n",
        "    for sentence in sentences:\n",
        "        sentence = [word.lower() for word in sentence if word.isalpha()]\n",
        "        preprocessed.append(sentence)\n",
        "    return preprocessed\n",
        "\n",
        "preprocessed_sentences = preprocess_sentences(europarl_sentences)\n",
        "\n",
        "# Tokenización:\n",
        "def tokenize(text):\n",
        "  tokens = [[re.sub(r'[^\\w\\s]', '',t.lower()) for t in tex] for tex in text]\n",
        "  return tokens\n",
        "\n",
        "# Lematización\n",
        "def lemmatize(tokens):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [[lemmatizer.lemmatize(tok) for tok in token] for token in tokens]\n",
        "  return lemmatized_tokens\n",
        "\n",
        "# Stemming\n",
        "def stemming(tokens):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_tokens = [[stemmer.stem(tok) for tok in token] for token in tokens]\n",
        "  return stemmed_tokens\n",
        "\n",
        "# Remoción de stopwords\n",
        "stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "# Filter extrange words\n",
        "# En este caso se eliminarán las palabras que aparezcan menos de 5 veces\n",
        "# para reducir el tamaño del vocabulario\n",
        "def reduce_vocabulary_by_frequency(sentences, threshold=5):\n",
        "  word_counts = defaultdict(int)\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      word_counts[word] += 1\n",
        "\n",
        "  new_sentences = []\n",
        "  for sentence in sentences:\n",
        "    new_sentence = [word for word in sentence if word_counts[word] >= threshold]\n",
        "    new_sentences.append(new_sentence)\n",
        "  return new_sentences\n",
        "\n",
        "# Generar n-gramas\n",
        "def generate_ngrams(sentence, n):\n",
        "    ngrams = zip(*[sentence[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "tokenized_sentences = remove_stop_words(tokenize(stemming(lemmatize(preprocessed_sentences[:500]))))\n",
        "\n",
        "print(tokenized_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sortedcontainers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObGjDqLeQZa8",
        "outputId": "7fc15e27-c0f6-4b3c-9ea5-8df54471f52a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sortedcontainers\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers\n",
            "Successfully installed sortedcontainers-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "AcVggqSqkOFI",
        "outputId": "6e6e74a2-0eef-4882-ec18-3ba82883bb77"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'europarl_sentences' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-00b7ed10abe4>\u001b[0m in \u001b[0;36m<cell line: 135>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m# Preprocesamos el corpus y ejecutamos el algoritmo de Brown Clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0mpreprocessed_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meuroparl_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Limitamos el corpus a las primeras 500 oraciones para simplificar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0mfinal_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'europarl_sentences' is not defined"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "from sortedcontainers import SortedDict\n",
        "\n",
        "# Implementación de técnicas:\n",
        "\n",
        "# A. Brown clustering\n",
        "\n",
        "# Implementar el algoritmo de Brown Clustering para agrupar palabras basándose en su contexto\n",
        "\n",
        "def preprocess_sentences(sentences):\n",
        "    preprocessed = []\n",
        "    for sentence in sentences:\n",
        "        sentence = [word.lower() for word in sentence if word.isalpha()]\n",
        "        preprocessed.append(sentence)\n",
        "    return preprocessed\n",
        "\n",
        "# Inicializamos los clusters y\n",
        "# asignamos cada palabra a su propio cluster\n",
        "def initialize_clusters(corpus):\n",
        "    clusters = {}\n",
        "    cluster_id = 0\n",
        "    word_frequency = defaultdict(int)\n",
        "\n",
        "    for sentence in corpus:\n",
        "        for word in sentence:\n",
        "            if word not in clusters:\n",
        "                clusters[word] = cluster_id\n",
        "                cluster_id += 1\n",
        "            word_frequency[word] += 1\n",
        "    return clusters, word_frequency\n",
        "\n",
        "\n",
        "# Función para calcular la probabilidad P(c) y P(ci, cj) a partir del corpus procesado\n",
        "def calculate_probabilities(corpus, clusters):\n",
        "    cluster_freq = defaultdict(int) # Inicializamos un diccionario con valores por defecto\n",
        "    bigram_freq = defaultdict(int) # Inicializamos un diccionario con valores por defecto\n",
        "    total_bigrams = 0\n",
        "\n",
        "    for sentence in corpus:\n",
        "        for i in range(len(sentence) - 1):\n",
        "            word1, word2 = sentence[i], sentence[i + 1]\n",
        "            cluster1, cluster2 = clusters[word1], clusters[word2]\n",
        "            cluster_freq[cluster1] += 1\n",
        "            bigram_freq[(cluster1, cluster2)] += 1\n",
        "            total_bigrams += 1\n",
        "\n",
        "    P_c = {c: freq / total_bigrams for c, freq in cluster_freq.items()}\n",
        "    P_c_c = {(c1, c2): freq / total_bigrams for (c1, c2), freq in bigram_freq.items()}\n",
        "\n",
        "    return P_c, P_c_c\n",
        "\n",
        "\n",
        "# Calcular la información mutua para el estado actual de los clusters\n",
        "def compute_mutual_information(P_c, P_c_c):\n",
        "  # Esta información es una medida de la dependencia entre pares de clusters\n",
        "    MI = 0\n",
        "    for (c1, c2), P_cc in P_c_c.items():\n",
        "        if P_c[c1] > 0 and P_c[c2] > 0:\n",
        "            MI += P_cc * math.log(P_cc / (P_c[c1] * P_c[c2]), 2)\n",
        "    return MI\n",
        "\n",
        "# Buscamos la mejor fusión que maximiza la ganancia de información mutua\n",
        "def find_best_merge(P_c, P_c_c, max_candidates=10):\n",
        "\n",
        "    # En esta función se evalúa todos los pares posibles de clusters\n",
        "    # y se calcula la reducción de información mutua si se fusionan dichos pares\n",
        "    # Lugeo, se selecciona el par que minimiza dicha pérdida en la información\n",
        "    # mutua\n",
        "\n",
        "    best_pair = None\n",
        "    best_increase = float('-inf')\n",
        "    candidate_pairs = SortedDict()\n",
        "\n",
        "    for (c1, c2) in P_c_c.keys():\n",
        "        merged_P_c = P_c.copy()\n",
        "        merged_P_c_c = P_c_c.copy()\n",
        "\n",
        "        # Fusionamos las probabilidades de c1 y c2\n",
        "        merged_P_c[c1] += merged_P_c.pop(c2, 0)\n",
        "        for (ci, cj) in list(merged_P_c_c.keys()):\n",
        "            if cj == c2:\n",
        "                merged_P_c_c[(ci, c1)] = merged_P_c_c.pop((ci, cj), 0) + merged_P_c_c.get((ci, c1), 0)\n",
        "            elif ci == c2:\n",
        "                merged_P_c_c[(c1, cj)] = merged_P_c_c.pop((ci, cj), 0) + merged_P_c_c.get((c1, cj), 0)\n",
        "\n",
        "        # Calcular el aumento en información mutua como la diferencia\n",
        "        # entre la información mutua después y antes de la fusión\n",
        "        MI_increase = compute_mutual_information(merged_P_c, merged_P_c_c) - compute_mutual_information(P_c, P_c_c)\n",
        "\n",
        "        #if MI_increase > best_increase:\n",
        "         #   best_pair = (c1, c2)\n",
        "          #  best_increase = MI_increase\n",
        "        if len(candidate_pairs) < max_candidates or MI_increase > min(candidate_pairs):\n",
        "          candidate_pairs[(MI_increase, (c1, c2))] = (c1, c2)\n",
        "          if len(candidate_pairs) > max_candidates:\n",
        "              candidate_pairs.popitem(index=0)\n",
        "\n",
        "    if candidate_pairs:\n",
        "        best_increase, best_pair = candidate_pairs.popitem(index=-1)\n",
        "\n",
        "    return best_pair\n",
        "\n",
        "\n",
        "# Fusión de clusters\n",
        "def merge_clusters(clusters, best_pair):\n",
        "    c1, c2 = best_pair\n",
        "    for word, cluster in clusters.items():\n",
        "        if cluster == c2:\n",
        "            clusters[word] = c1\n",
        "    return clusters\n",
        "\n",
        "# Algoritmo principal de Brown Clustering\n",
        "def brown_clustering(corpus, target_clusters=100):\n",
        "\n",
        "    # Como ya sabemos este algoritmo agrupará en un número\n",
        "    # objetivo de clusters basándonos en el contexto en el que aparecen en\n",
        "    # un corpus dado\n",
        "\n",
        "    # Asignamos cada palabra a su propio cluster y calcula la frecuencia de cada palabra\n",
        "    clusters, word_frequency = initialize_clusters(corpus)\n",
        "    P_c, P_c_c = calculate_probabilities(corpus, clusters)\n",
        "\n",
        "    while len(set(clusters.values())) > target_clusters:\n",
        "      #Encontramos el mejor par de clusters para fusionar\n",
        "        best_pair = find_best_merge(P_c, P_c_c)\n",
        "        if best_pair is None:\n",
        "            break\n",
        "        clusters = merge_clusters(clusters, best_pair)\n",
        "        P_c, P_c_c = calculate_probabilities(corpus, clusters)\n",
        "\n",
        "    return clusters\n",
        "\n",
        "# Preprocesamos el corpus y ejecutamos el algoritmo de Brown Clustering\n",
        "preprocessed_sentences = preprocess_sentences(europarl_sentences[:500]) # Limitamos el corpus a las primeras 500 oraciones para simplificar\n",
        "final_clusters = brown_clustering(preprocessed_sentences, target_clusters=100)\n",
        "\n",
        "print(final_clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFPQVGqskYhm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse.linalg import svds\n",
        "# B. Latent semantic analysis (LSA)\n",
        "\n",
        "# Aplicar LSA para reducir la dimensionalidad y capturar\n",
        "# relaciones semánticas\n",
        "\n",
        "# Paso 1: Construcción de la matriz temrino-documento\n",
        "def construct_term_document_matrix(corpus):\n",
        "  # Creamos la matriz usando el método TF-IDF\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(corpus)  # Matriz TF-IDF\n",
        "  return X.toarray(), vectorizer.get_feature_names_out()\n",
        "\n",
        "# Paso 2: Implementación de SVD\n",
        "def perform_svd(X, k):\n",
        "  # Aquí realizamos la descomposición en valores singulares de la amtriz X\n",
        "\n",
        "  # Param X: Vendría a ser la matriz termino-documento\n",
        "  # Param K: número de valoers singualres a conservar\n",
        "  U, sigma, VT = svds(X, k=k)\n",
        "  return U, sigma, VT\n",
        "\n",
        "# Paso 3: Reducción de la dimensionalidad\n",
        "def reduce_dimensions(U, sigma, VT):\n",
        "  S = np.diag(sigma)\n",
        "  X_k = np.dot(U, S)\n",
        "  return X_k, VT\n",
        "\n",
        "# Paso 4: Proyeccipon: Representar términos y documentos en el espacio reducido\n",
        "def represent_terms_documents(X_k, VT):\n",
        "    #:param X_k: Matriz reducida de documentos.\n",
        "    #:param VT: Matriz V^T de SVD.\n",
        "    return X_k, VT.T  # Transponemos VT para tener términos en filas\n",
        "\n",
        "# Función principal para LSA\n",
        "def latent_semantic_analysis(corpus, k):\n",
        "    # Paso 1: Construcción de la matriz término-documento\n",
        "    X, terms = construct_term_document_matrix(corpus)\n",
        "\n",
        "    # Paso 2: Implementación SVD\n",
        "    U, sigma, VT = perform_svd(X, k)\n",
        "\n",
        "    # Paso 3: Reducción de la dimensionalidad\n",
        "    X_k, VT_reduced = reduce_dimensions(U, sigma, VT)\n",
        "\n",
        "    # Paso 4: Proyeccipon: Representar términos y documentos en el espacio reducido\n",
        "    term_representation, document_representation = represent_terms_documents(X_k, VT_reduced)\n",
        "\n",
        "    return term_representation, document_representation, terms\n",
        "\n",
        "k = 2  # Dimensiones a conservar\n",
        "term_representation, document_representation, terms = latent_semantic_analysis(europarl_sentences[:500], k)\n",
        "\n",
        "print(\"Representación de términos:\\n\", term_representation)\n",
        "print(\"Representación de documentos:\\n\", document_representation)\n",
        "print(\"Términos:\\n\", terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LadjeWEio-NR"
      },
      "outputs": [],
      "source": [
        "# C. Word2Vec\n",
        "preprocessed_sentences = preprocess_sentences(europarl_sentences)\n",
        "\n",
        "# Construcción del vocabulario\n",
        "def build_vocab(sentences, min_count=2):\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    vocab = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_count}\n",
        "    reverse_vocab = {i: word for word, i in vocab.items()}\n",
        "    return vocab, reverse_vocab\n",
        "\n",
        "vocab, reverse_vocab = build_vocab(preprocessed_sentences)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Paso 2: Generación de pares (contexto, palabra objetivo) para CBOW\n",
        "def generate_context_target_pairs(sentences, vocab, window_size=2):\n",
        "    pairs = []\n",
        "    for sentence in sentences:\n",
        "        word_indices = [vocab[word] for word in sentence if word in vocab]\n",
        "        for i, target in enumerate(word_indices):\n",
        "            # Definir la ventana de contexto de tamaño 'window_size'\n",
        "            context = [\n",
        "                word_indices[j]\n",
        "                for j in range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
        "                if j != i\n",
        "            ]\n",
        "            pairs.append((context, target))\n",
        "    return pairs\n",
        "\n",
        "context_target_pairs = generate_context_target_pairs(preprocessed_sentences, vocab)\n",
        "\n",
        "# Paso 3: Implementación del modelo CBOW\n",
        "class Word2VecCBOW:\n",
        "    def __init__(self, vocab_size, embedding_dim=50, lr=0.03):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lr = lr\n",
        "        # Incializamos las matrices de pesos\n",
        "        self.W1 = np.random.randn(vocab_size, embedding_dim) * 0.01  # embeddings\n",
        "        self.W2 = np.random.randn(embedding_dim, vocab_size) * 0.01  # proyección para las ssalidas\n",
        "\n",
        "    def forward(self, context_words):\n",
        "        h = np.mean(self.W1[context_words], axis=0)\n",
        "        u = np.dot(h, self.W2)  # Proyección al espacio de salida\n",
        "        y_pred = self.softmax(u)  # Probabilidades de predicción\n",
        "        return y_pred, h\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Lo usaremos para nroamlizar las probabilidades\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def negative_log_loss(self, y_pred, target):\n",
        "        return -np.log(y_pred[target] + 1e-9)\n",
        "\n",
        "    def backward(self, y_pred, target, h, context_words):\n",
        "        # Optimización: Algoritmo básido de descenso de gradiente\n",
        "        # para actualizar los vectores de palabras\n",
        "        error = y_pred\n",
        "        error[target] -= 1\n",
        "        grad_W2 = np.outer(h, error)  # Gradiente para W2\n",
        "        grad_h = np.dot(self.W2, error)  # Gradiente para la capa oculta\n",
        "        grad_W1 = np.zeros_like(self.W1)\n",
        "\n",
        "        for word in context_words:\n",
        "            grad_W1[word] += grad_h  # Suma del gradiente para palabras de contexto\n",
        "\n",
        "        # Actualización de pesos\n",
        "        self.W1[context_words] -= self.lr * grad_W1[context_words]\n",
        "        self.W2 -= self.lr * grad_W2\n",
        "\n",
        "    def train(self, pairs, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            loss = 0\n",
        "            for context_words, target in pairs:\n",
        "                # Calculamos las predicciones\n",
        "                y_pred, h = self.forward(context_words)\n",
        "                # Calculamos la pérdida logarítmica negativa\n",
        "                loss += self.negative_log_loss(y_pred, target)\n",
        "                # Backward pass: Actualización de pesos con descenso de gradiente\n",
        "                self.backward(y_pred, target, h, context_words)\n",
        "            print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Entrenamiento el modelo CBOW\n",
        "embedding_dim = 50\n",
        "model = Word2VecCBOW(vocab_size, embedding_dim)\n",
        "model.train(context_target_pairs, epochs=5)\n",
        "\n",
        "words_to_test = [\"europa\", \"política\", \"economía\"] # Plabras del corpus europl en español\n",
        "for word in words_to_test:\n",
        "    if word in vocab:\n",
        "        idx = vocab[word]\n",
        "        print(f\"Embedding para '{word}': {model.W1[idx]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "# <------------------------------------------------>\n",
        "# Implementación de Skip-gram con negative sampling\n",
        "\n",
        "# Construcción del vocabulario, reutilizamos la misma función que creamos para el CBOW\n",
        "def build_vocab(sentences, min_count=2):\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    vocab = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_count}\n",
        "    reverse_vocab = {i: word for word, i in vocab.items()}\n",
        "    return vocab, reverse_vocab, word_counts\n",
        "\n",
        "vocab, reverse_vocab, word_counts = build_vocab(preprocessed_sentences)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Paso 2: Generación de pares (contexto, palabra objetivo) para CBOW\n",
        "def generate_skipgram_pairs(sentences, vocab, window_size=2):\n",
        "    pairs = []\n",
        "    for sentence in sentences:\n",
        "        word_indices = [vocab[word] for word in sentence if word in vocab]\n",
        "        for i, target in enumerate(word_indices):\n",
        "            # Definimos el contexto dentro de la ventana de tamaño `window_size`\n",
        "            context_indices = [\n",
        "                word_indices[j]\n",
        "                for j in range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
        "                if j != i\n",
        "            ]\n",
        "            for context in context_indices:\n",
        "                pairs.append((target, context))\n",
        "    return pairs\n",
        "\n",
        "skipgram_pairs = generate_skipgram_pairs(preprocessed_sentences, vocab)\n",
        "\n",
        "# Paso 3: Implementamos el Negative Sampling\n",
        "def negative_sampling_distribution(word_counts, vocab, power=0.75):\n",
        "    # Cálculo de la distribución ajustada para el muestreo negativo\n",
        "    word_freq = np.array([word_counts[reverse_vocab[i]] for i in range(len(vocab))])\n",
        "    unigram_dist = word_freq**power\n",
        "    unigram_dist /= unigram_dist.sum()\n",
        "    return unigram_dist\n",
        "\n",
        "unigram_dist = negative_sampling_distribution(word_counts, vocab)\n",
        "\n",
        "def get_negative_samples(target, num_neg_samples=5):\n",
        "    # Selección de muestras negativas según la distribución\n",
        "    negative_samples = []\n",
        "    while len(negative_samples) < num_neg_samples:\n",
        "        neg_sample = np.random.choice(range(vocab_size), p=unigram_dist)\n",
        "        if neg_sample != target:\n",
        "            negative_samples.append(neg_sample)\n",
        "    return negative_samples\n",
        "\n",
        "# Modelo Skip-Gram con Negative Sampling\n",
        "class SkipGramNegSampling:\n",
        "    def __init__(self, vocab_size, embedding_dim=50, lr=0.03):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lr = lr\n",
        "        self.W1 = np.random.randn(vocab_size, embedding_dim) * 0.01  # Embedding de entrada\n",
        "        self.W2 = np.random.randn(embedding_dim, vocab_size) * 0.01  # Proyección de salida\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def loss(self, target, context, negative_samples):\n",
        "        # Cálculo de pérdida para cada par objetivo-contexto y las muestras negativas\n",
        "        pos_score = np.dot(self.W1[target], self.W2[:, context])\n",
        "        pos_loss = -np.log(self.sigmoid(pos_score))\n",
        "\n",
        "        neg_loss = 0\n",
        "        for neg in negative_samples:\n",
        "            neg_score = np.dot(self.W1[target], self.W2[:, neg])\n",
        "            neg_loss += -np.log(self.sigmoid(-neg_score))\n",
        "\n",
        "        return pos_loss + neg_loss\n",
        "\n",
        "    def train_pair(self, target, context):\n",
        "        # Negative Sampling: Generación de muestras negativas\n",
        "        negative_samples = get_negative_samples(target, num_neg_samples=5)\n",
        "\n",
        "        pos_score = np.dot(self.W1[target], self.W2[:, context])\n",
        "        neg_scores = np.dot(self.W1[target], self.W2[:, negative_samples])\n",
        "\n",
        "        # Cálculo de gradientes usando backward\n",
        "        pos_grad = self.sigmoid(pos_score) - 1\n",
        "        self.W1[target] -= self.lr * pos_grad * self.W2[:, context]\n",
        "        self.W2[:, context] -= self.lr * pos_grad * self.W1[target]\n",
        "\n",
        "        for neg, neg_score in zip(negative_samples, neg_scores):\n",
        "            neg_grad = self.sigmoid(-neg_score) - 1\n",
        "            self.W1[target] -= self.lr * neg_grad * self.W2[:, neg]\n",
        "            self.W2[:, neg] -= self.lr * neg_grad * self.W1[target]\n",
        "\n",
        "        return self.loss(target, context, negative_samples)\n",
        "\n",
        "    def train(self, pairs, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for target, context in pairs:\n",
        "                # Entrenamiento de cada par con pérdida negativa logarítmica\n",
        "                total_loss += self.train_pair(target, context)\n",
        "            print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Entrenamos del modelo\n",
        "embedding_dim = 50\n",
        "model = SkipGramNegSampling(vocab_size, embedding_dim)\n",
        "model.train(skipgram_pairs, epochs=5)\n",
        "\n",
        "words_to_test = [\"europa\", \"política\", \"economía\"] # Plabras del corpus europl en español\n",
        "for word in words_to_test:\n",
        "    if word in vocab:\n",
        "        idx = vocab[word]\n",
        "        print(f\"Embedding para '{word}': {model.W1[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "Maa_LhmApoxb",
        "outputId": "bc43f85a-4f6b-4d03-ddfe-213743482d21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocessed_sentences' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f38788f8300b>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7kgx9TCo97m"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "# D. GloVe (Global vectors for word representation)\n",
        "\n",
        "# En este caso nos piden entrenar el modelo GloVe para obtener\n",
        "# embeddings de palabras basados en co-ocurrencias globables\n",
        "\n",
        "# Construimos la matriz de co-ocurrencia\n",
        "def build_cooccurrence_matrix(sentences, vocab, window_size=5):\n",
        "    cooccurrence = defaultdict(Counter)\n",
        "    for sentence in sentences:\n",
        "        indices = [vocab[word] for word in sentence if word in vocab]\n",
        "        for i, target in enumerate(indices):\n",
        "            # Recorre la ventana de contexto\n",
        "            start = max(i - window_size, 0)\n",
        "            end = min(i + window_size + 1, len(indices))\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    cooccurrence[target][indices[j]] += 1.0 / abs(i - j)\n",
        "    return cooccurrence\n",
        "\n",
        "def build_vocab(sentences, min_count=2):\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    vocab = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_count}\n",
        "    reverse_vocab = {i: word for word, i in vocab.items()}\n",
        "    return vocab, reverse_vocab, word_counts\n",
        "\n",
        "vocab, reverse_vocab, word_counts = build_vocab(preprocessed_sentences)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "cooccurrence_matrix = build_cooccurrence_matrix(preprocessed_sentences, vocab)\n",
        "\n",
        "\n",
        "# Inicializamos los vectores y sesgos: Asignar vectores y términos de sesgo ini-\n",
        "# ciales a cada palabra de manera aleatoria.\n",
        "embedding_dim = 50  # Dimensionalidad del embedding\n",
        "word_vectors = np.random.rand(vocab_size, embedding_dim)\n",
        "context_vectors = np.random.rand(vocab_size, embedding_dim)\n",
        "bias_word = np.random.rand(vocab_size)\n",
        "bias_context = np.random.rand(vocab_size)\n",
        "\n",
        "# Función de ponderación f(X)\n",
        "def weighting_function(x, x_max=100, alpha=0.75):\n",
        "    if x < x_max:\n",
        "        return (x / x_max) ** alpha\n",
        "    return 1\n",
        "\n",
        "# Paso 3: Función de costo y optimización con descenso de gradiente estocástico\n",
        "def glove_loss(target, context, count, x_max=100, alpha=0.75, learning_rate=0.05):\n",
        "    # Calcula la función de pérdida de GloVe\n",
        "    weight = weighting_function(count, x_max, alpha)\n",
        "    diff = np.dot(word_vectors[target], context_vectors[context]) + bias_word[target] + bias_context[context] - np.log(count)\n",
        "    loss = weight * (diff ** 2)\n",
        "\n",
        "    # Definimos las Gradientes\n",
        "    grad = 2 * weight * diff\n",
        "    grad_word_vector = grad * context_vectors[context]\n",
        "    grad_context_vector = grad * word_vectors[target]\n",
        "\n",
        "    # Actualización de vectores y sesgos usando SGD (Descenso de gradiente estocástico)\n",
        "    word_vectors[target] -= learning_rate * grad_word_vector\n",
        "    context_vectors[context] -= learning_rate * grad_context_vector\n",
        "    bias_word[target] -= learning_rate * grad\n",
        "    bias_context[context] -= learning_rate * grad\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Paso 4: Iteración hasta convergencia: el proceso de optimización hasta que la\n",
        "# función de costo converja o se alcance un número máximo de iteraciones.\n",
        "def train_glove(cooccurrence_matrix, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for target, contexts in cooccurrence_matrix.items():\n",
        "            for context, count in contexts.items():\n",
        "                # Actualización y cálculo de la pérdida\n",
        "                loss = glove_loss(target, context, count)\n",
        "                total_loss += loss\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "train_glove(cooccurrence_matrix)\n",
        "\n",
        "# Opcional: Optimización adicional (Mejoras en complejidad)\n",
        "\n",
        "# Paralelización para actualizar múltiples vectores simultáneamente\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def parallel_train_glove(cooccurrence_matrix, epochs=50, n_threads=4):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
        "            futures = []\n",
        "            for target, contexts in cooccurrence_matrix.items():\n",
        "                for context, count in contexts.items():\n",
        "                    futures.append(executor.submit(glove_loss, target, context, count))\n",
        "            total_loss = sum(f.result() for f in futures)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Optimización de la función de ponderación\n",
        "# Precomputar valores de la función de ponderación para acelerar el cálculo\n",
        "x_max = 100\n",
        "alpha = 0.75\n",
        "weight_cache = {x: weighting_function(x, x_max, alpha) for x in range(1, x_max + 1)}\n",
        "\n",
        "def optimized_weighting_function(x):\n",
        "    return weight_cache.get(x, 1)\n",
        "\n",
        "\n",
        "def glove_loss_optimized(target, context, count, x_max=100, alpha=0.75, learning_rate=0.05):\n",
        "    # Calcula la función de pérdida de GloVe\n",
        "    weight = optimized_weighting_function(count) # Aquí hemos reemplazado el weighting_funcion\n",
        "    # por el optimized_weighting_function para. utilizar la optimización\n",
        "    diff = np.dot(word_vectors[target], context_vectors[context]) + bias_word[target] + bias_context[context] - np.log(count)\n",
        "    loss = weight * (diff ** 2)\n",
        "\n",
        "    # Definimos las Gradientes\n",
        "    grad = 2 * weight * diff\n",
        "    grad_word_vector = grad * context_vectors[context]\n",
        "    grad_context_vector = grad * word_vectors[target]\n",
        "\n",
        "    # Actualización de vectores y sesgos usando SGD (Descenso de gradiente estocástico)\n",
        "    word_vectors[target] -= learning_rate * grad_word_vector\n",
        "    context_vectors[context] -= learning_rate * grad_context_vector\n",
        "    bias_word[target] -= learning_rate * grad\n",
        "    bias_context[context] -= learning_rate * grad\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tujS_5HLOeFG",
        "outputId": "f0c6de7c-46b8-4876-bdb5-583dfe575879"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n",
            "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiempo de ejecución y uso de memoria\n",
        "import time\n",
        "import memory_profiler\n",
        "\n",
        "start_time = time.time()\n",
        "train_glove(cooccurrence_matrix)\n",
        "end_time = time.time()\n",
        "print(f\"Tiempo de ejecución: {end_time - start_time:.2f} segundos\")\n",
        "\n",
        "# Medición de uso de memoria\n",
        "@memory_profiler.profile\n",
        "def run_memory_intensive_task():\n",
        "    train_glove(cooccurrence_matrix)\n",
        "\n",
        "run_memory_intensive_task()"
      ],
      "metadata": {
        "id": "7-NhD0-KOdtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yJHG5x4C1hk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bGmpf6fuLAaz"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}